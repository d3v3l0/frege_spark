module examples.IntegrationRDDExperiment where

import bindings.spark.SparkConf
--import bindings.custom.ImpureCustomFunction
import bindings.Function
import bindings.custom.CustomFunction
import bindings.custom.CustomFunction2
import bindings.testing.TestBindings
import bindings.spark.JavaSparkContext
import bindings.spark.JavaRDD
import config.Config


filterThreeOrFive :: Double -> Bool
filterThreeOrFive 3 = True
filterThreeOrFive 5 = True
filterThreeOrFive _ = false

main :: IO ()
main = do
  sparkConfig <- SparkConf.new ()
  println fregecJar
  println "IntegrationRDDExperiment"
  list <- arrayFromListST [applicationJar, fregecJar, interpreterJar]
  sparkConfig.setAppName "Frege-Spark"
  sparkConfig.setJars list
  sparkConfig.setMaster "local"
  
  conf <- SparkConf.new () >>= _.setMaster "local" >>= _.setAppName "Frege-Spark" >>= _.setJars list

  --sparkConfig.setMaster "spark://Damians-MacBook.local:7077"
  sc :: MutableIO JavaSparkContext <- JavaSparkContext.new sparkConfig
  currentData2 <- JavaSparkContext.textFile sc "data/first.csv"

  currentData <- JavaSparkContext.textFile sc "data/first.csv"
  parsedData = currentData.map  CustomFunction.Function.convertToDouble
  --filteredData = parsedData.filter (CustomFunction.Function.createFilterFunction filterThreeOrFive)
  --filteredData = parsedData.filter (CustomFunction.Function.createFilterFunction filterThreeOrFive)
  -- -> causes Exception in thread "main" org.apache.spark.SparkException: Task not serializable
  --           Caused by: java.io.NotSerializableException: examples.frege.numbers.IntegrationRDDExample$$Lambda$68/1281361915
  --mappedData = filteredData.map  CustomFunction.Function.timesTenDouble
  mappedData2 = parsedData.map $ Function.Function.createInterpretedFunction "impureFunction"
  sum = mappedData2.count --CustomFunction2.Function2.getSum
  
  --filteredData = JavaRDD.filter (JavaRDD.map currentData2 Function.convertToDouble) (Function.createFilterFunction filterThreeOrFive)

  --mappedData = parsedData.map $ Function.createIOFunction ()

  --currentData3 <- JavaSparkContext.textFile sc "data/first.csv " >>= _.map  Function.convertToDouble

  --parsedData.filter (Function.createFilterFunction filterThreeOrFive) >>= _.reduce Function2.getSum
  
  currentFirst = currentData.first
  assertEquals "1" currentFirst

  --parsedFirst = parsedData.first
  --assertEquals 1.0 parsedFirst
  
  --mappedData2First = mappedData2.first
  --assertEquals 1.0 mappedData2First
    
  {-filteredFirst = filteredData.first
  assertEquals 3.0 filteredFirst

  mappedFirst = mappedData.first
  assertEquals 30.0 mappedFirst 

  assertEquals 80.0 sum
-}
  --assertEquals 80.0 sum
  assertEquals 8L sum

  scMaster :: String <- sc.master
  println $ "calculated with master: " ++ scMaster
  println "finished IntegrationRDDExperiment"
